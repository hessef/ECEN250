{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtRIzehBQ05c"
   },
   "source": [
    "#Part 1: Linear Regression and Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il9_-m0lEuTt"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUWb3_xmFGxO"
   },
   "source": [
    "In this lecture, we will work with the `vehicles` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vHtAgEjEuOU"
   },
   "outputs": [],
   "source": [
    "vehicles = sns.load_dataset(\"mpg\").rename(columns={\"horsepower\":\"hp\"}).dropna().sort_values(\"hp\")\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-LWfGFqIU0V"
   },
   "outputs": [],
   "source": [
    "vehicles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSFBaLc_FMNf"
   },
   "source": [
    "We will attempt to predict a car's \"mpg\" from transformations of its \"hp\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc36mf8_EuLT"
   },
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "X[\"hp^2\"] = vehicles[\"hp\"]**2\n",
    "X[\"hp^3\"] = vehicles[\"hp\"]**3\n",
    "X[\"hp^4\"] = vehicles[\"hp\"]**4\n",
    "\n",
    "Y = vehicles[\"mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js-iQwLBFO0u"
   },
   "source": [
    "Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJkN9mxZFQgx"
   },
   "source": [
    "To perform a train-test split, we can use the train_test_split function of the sklearn.model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnS3YaZ9EuI1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# `test_size` specifies the proportion of the full dataset that should be allocated to testing.\n",
    "# `random_state` makes our results reproducible for educational purposes.\n",
    "# shuffle is True by default and randomizes the data before splitting.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=100,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "print(f\"Size of full dataset: {X.shape[0]} points\")\n",
    "print(f\"Size of training set: {X_train.shape[0]} points\")\n",
    "print(f\"Size of test set: {X_test.shape[0]} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFk9TN7BFU49"
   },
   "source": [
    "We then fit the model using the training set...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_ugV_pxEt-p"
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "model = lm.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c4THBOBKdI-"
   },
   "source": [
    "**Insert a code block below to train the model with the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hOkEtqCFXLU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w81CQDc3LdYW"
   },
   "source": [
    "**Insert a code block below, make predictions on both training set and test set, and print the mean squared error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b78DvFGAL0ci"
   },
   "source": [
    "**Insert a txt block below and answer the question: Try to explain why the model performs more poorly on the test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQECnzzYFc9E"
   },
   "source": [
    "Validation Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR9Ptz_oFgBH"
   },
   "source": [
    "To assess model performance on unseen data, then use this information to finetune the model, we introduce a validation set. You can imagine this as us splitting the training set into a validation set and a \"mini\" training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeL8nDNYFXIZ"
   },
   "outputs": [],
   "source": [
    "# Split X_train further into X_train_mini and X_val.\n",
    "X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k94_NWYMn8S"
   },
   "source": [
    "**Insert a code cell below to print the size of original training set, mini training set, and validation set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uiekv2kWFiil"
   },
   "source": [
    "In the cell below, we fit several models of increasing complexity, then compute their errors. Here, we find the model's errors on the validation set to understand how model complexity influences performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qvHpUhmFXF0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for order in [2, 3, 4]:\n",
    "    model = lm.LinearRegression()\n",
    "    model.fit(X_train_mini.iloc[:, :order], Y_train_mini)\n",
    "    val_predictions = model.predict(X_val.iloc[:, :order])\n",
    "\n",
    "    output = X_val.iloc[:, :order]\n",
    "    output[\"y_hat\"] = val_predictions\n",
    "    output = output.sort_values(\"hp\")\n",
    "\n",
    "    ax[order-2].scatter(X_val[\"hp\"], Y_val, edgecolor=\"white\", lw=0.5)\n",
    "    ax[order-2].plot(output[\"hp\"], output[\"y_hat\"], \"tab:red\")\n",
    "    ax[order-2].set_title(f\"Model with degree {order}\")\n",
    "    ax[order-2].set_xlabel(\"hp\")\n",
    "    ax[order-2].set_ylabel(\"mpg\")\n",
    "    ax[order-2].annotate(f\"Validation MSE: {np.round(mean_squared_error(Y_val, val_predictions), 3)}\", (90, 30))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApuR5yOEFmJq"
   },
   "source": [
    "Let's repeat this process:\n",
    "\n",
    "1. Fit an degree-x model to the mini training set\n",
    "2. Evaluate the fitted model's MSE when making predictions on the validation set.  \n",
    "We use the model's performance on the validation set as a guide to selecting the best combination of features. We are not limited in the number of times we use the validation set â€“ we just never use this set to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAGJ669ZFXDO"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_model_dataset(degree):\n",
    "    pipelined_model = Pipeline([\n",
    "            ('polynomial_transformation', PolynomialFeatures(degree)),\n",
    "            ('linear_regression', lm.LinearRegression())\n",
    "        ])\n",
    "\n",
    "    pipelined_model.fit(X_train_mini[[\"hp\"]], Y_train_mini)\n",
    "    return mean_squared_error(Y_val, pipelined_model.predict(X_val[[\"hp\"]]))\n",
    "\n",
    "errors = [fit_model_dataset(degree) for degree in range(0, 18)]\n",
    "MSEs_and_k = pd.DataFrame({\"k\": range(0, 18), \"MSE\": errors})\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "plt.plot(range(0, 18), errors)\n",
    "plt.xlabel(\"Model Complexity (degree of polynomial)\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.xticks(range(0, 18));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhMmuTzhPc1k"
   },
   "source": [
    "**Insert a txt block and answer the question: Looking at the figure above, what values of degree of polynomial lead to underfitting and overfitting of the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwlZf5KmFXAp"
   },
   "outputs": [],
   "source": [
    "MSEs_and_k.rename(columns={\"k\":\"Degree\"}).set_index(\"Degree\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EwLpeYBFr7Z"
   },
   "source": [
    "From this model selection process, we might choose to create a model with degree 8.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq6r4meJFtb3"
   },
   "outputs": [],
   "source": [
    "print(f'Polynomial degree with lowest validation error: {MSEs_and_k.sort_values(\"MSE\").head(1)[\"k\"].values}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37zH-4bpFvEw"
   },
   "source": [
    "After this choice has been finalized, and we are completely finished with the model design process, we finally assess model performance on the test set. We typically use the entire training set (both the \"mini\" training set and validation set) to fit the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOBJeZExQjV-"
   },
   "outputs": [],
   "source": [
    "# Update our training and test sets to include all polynomial features between 5 and 9\n",
    "for degree in range(5, 9):\n",
    "    X_train[f\"hp^{degree}\"] = X_train[\"hp\"]**degree\n",
    "    X_test[f\"hp^{degree}\"] = X_test[\"hp\"]**degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8BY4nvMQNBI"
   },
   "source": [
    "**Insert code blocks below, train a linear regression model, and show the mean square error on the test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOoGbCLaFyYm"
   },
   "source": [
    "#Part 2: Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRNRgivwFz7Z"
   },
   "source": [
    "The validation set gave us an opportunity to understand how the model performs on a single set of unseen data. The specific validation set we drew was fixed â€“ we used the same validation points every time.\n",
    "\n",
    "It's possible that we may have, by random chance, selected a set of validation points that was not representative of other unseen data that the model might encounter (for example, if we happened to have selected all outlying data points for the validation set).\n",
    "\n",
    "Different train/validation splits lead to different validation errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f7uRClXvdae"
   },
   "source": [
    "**Add code in the code block below, create a for-loop with i ranges from 1 to 4, split the training set (X_train, Y_train) into train_mini set and val set with random state equals i, create a linear regression model, train the model, and print the mean square error on val set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hRID4WKFW91"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    ### Add your code here ###\n",
    "\n",
    "    ## Split the training set\n",
    "\n",
    "    ## Create a linear regression model\n",
    "\n",
    "    ## Train\n",
    "\n",
    "    ## Predict\n",
    "    y_hat = ...\n",
    "\n",
    "    ### End of your code ###\n",
    "    print(f\"Val error from train/validation split #{i}: {mean_squared_error(y_hat, Y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqn8RnzDF2z-"
   },
   "source": [
    "To apply cross-validation, we use the KFold class of sklearn.model_selection. KFold will return the indices of each cross-validation fold. Then, we iterate over each of these folds to designate it as the validation set, while training the model on the remaining folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH5jBe9wF4MN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(25) # Ensures reproducibility of this notebook\n",
    "\n",
    "# n_splits sets the number of folds to create\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "validation_errors = []\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    # Split the data\n",
    "    split_X_train, split_X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    split_Y_train, split_Y_valid = Y_train.iloc[train_idx], Y_train.iloc[valid_idx]\n",
    "\n",
    "    # Fit the model on the training split\n",
    "    model.fit(split_X_train, split_Y_train)\n",
    "\n",
    "    error = mean_squared_error(model.predict(split_X_valid), split_Y_valid)\n",
    "\n",
    "    validation_errors.append(error)\n",
    "\n",
    "print(f\"Cross-validation error: {np.mean(validation_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hv9Glm1xkfz"
   },
   "source": [
    "**Open txt blocks below, and answer the following question:**  \n",
    "1. How many folds do we split the training set?\n",
    "2. Based on the number of folds, how many percentages of training set (X_train) are split into split_X_train and split_X_valid? Add code to validate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYqRaCwxF5jg"
   },
   "source": [
    "#Part 3: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUM9Fn2nF654"
   },
   "source": [
    "L1 (LASSO) Regularization  \n",
    "To apply L1 regularization, we use the Lasso model class of sklearn. Lasso functions just like LinearRegression. The difference is that now the model will apply a regularization penalty. We specify the strength of regularization using the alpha parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": {
     "elapsed": 4840,
     "status": "error",
     "timestamp": 1753798951124,
     "user": {
      "displayName": "Kevin Nowka",
      "userId": "11083992144430275357"
     },
     "user_tz": 300
    },
    "id": "LFhGJdTyF8dY",
    "outputId": "9f981207-52c8-4dda-e1b5-0acd04030ee4"
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "lasso_model = lm.Lasso(alpha=0.1) # In sklearn, alpha represents the lambda hyperparameter\n",
    "lasso_model.fit(X_train, Y_train)\n",
    "\n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8lwoHvAF-QF"
   },
   "source": [
    "To increase the strength of regularization (decrease model complexity), we increase the\n",
    "Î»\n",
    " hyperparameter by changing alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66-rlMhs5RZM"
   },
   "source": [
    "**Insert a code block below, create a Lasso model named \"lasso_model_large_lambda\" with alpha=10, train the model, and print the coefficients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDQ7jdaVGBnC"
   },
   "source": [
    "Notice that these model coefficients are very small (some are effectively 0). This reflects L1 regularization's tendency to set the parameters of unimportant features to 0. We can use this in feature selection.\n",
    "\n",
    "The features in our dataset are on wildly different numerical scales. To see this, compare the values of hp to the values of hp^8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1753798951319,
     "user": {
      "displayName": "Kevin Nowka",
      "userId": "11083992144430275357"
     },
     "user_tz": 300
    },
    "id": "vI7lr6pMGC8j"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUdneUlXGM2L"
   },
   "source": [
    "In order for the feature hp to contribute in any meaningful way to the model, LASSO is \"forced\" to allocate disproportionately much of its parameter \"budget\" towards assigning a large value to the model parameter for hp. Notice how the parameter for hp is much, much greater in magnitude than the parameter for hp^8.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5099,
     "status": "aborted",
     "timestamp": 1753798951321,
     "user": {
      "displayName": "Kevin Nowka",
      "userId": "11083992144430275357"
     },
     "user_tz": 300
    },
    "id": "5HUbCKBPGN1z"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Feature\":X_train.columns, \"Parameter\":lasso_model.coef_})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dNcrjIwGO8O"
   },
   "source": [
    "We typically scale data before regularization such that all features are measured on the same numeric scale. One way to do this is by standardizing the data such that it has mean 0 and standard deviation 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5099,
     "status": "aborted",
     "timestamp": 1753798951322,
     "user": {
      "displayName": "Kevin Nowka",
      "userId": "11083992144430275357"
     },
     "user_tz": 300
    },
    "id": "tx1cMTUXGP5i"
   },
   "outputs": [],
   "source": [
    "# Center the data to have mean 0\n",
    "X_train_centered = X_train - X_train.mean()\n",
    "\n",
    "# Scale the centered data to have SD 1\n",
    "X_train_standardized = X_train_centered/X_train_centered.std()\n",
    "\n",
    "X_train_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk7FQY0pGRCM"
   },
   "source": [
    "When we re-fit a LASSO model, the coefficients are no longer as uneven in magnitude as they were before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVh_-mYu5w7n"
   },
   "source": [
    "**Insert a code cell below, create a Lasso model with alpha=0.1, train the model on the standardized set, and print the coefficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L-Ek_fAGUSN"
   },
   "source": [
    "L2 (Ridge) Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cilFQHReGXLW"
   },
   "source": [
    "We perform ridge regression using sklearn's Ridge class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5100,
     "status": "aborted",
     "timestamp": 1753798951324,
     "user": {
      "displayName": "Kevin Nowka",
      "userId": "11083992144430275357"
     },
     "user_tz": 300
    },
    "id": "BdhL4jJlFW62"
   },
   "outputs": [],
   "source": [
    "ridge_model = lm.Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train_standardized, Y_train)\n",
    "\n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb6X6pAD-yh2"
   },
   "source": [
    "**Insert a code cell below, print the mean squared error of the ridge model on the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4ua4E6J-woV"
   },
   "source": [
    "#Part 4: Using cross-validation to optimize regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spuSB6WCCo7b"
   },
   "source": [
    "**Add code in the code cell below, using cross-validation to find the best regularization parameter `alpha`**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjAGK3iH_ife"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(25) # Ensures reproducibility of this notebook\n",
    "\n",
    "# n_splits sets the number of folds to create\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "for alpha in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    validation_errors = []\n",
    "\n",
    "    ## Add your code here: create a Ridge model, with alpha=alpha\n",
    "    model = ...\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        # Split the data\n",
    "        split_X_train, split_X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "        split_Y_train, split_Y_valid = Y_train.iloc[train_idx], Y_train.iloc[valid_idx]\n",
    "\n",
    "\n",
    "        # Add your code here, fit the model on the training split\n",
    "\n",
    "\n",
    "        # Add your code here, calculate the mean square error on the validation set\n",
    "        error = ...\n",
    "\n",
    "        validation_errors.append(error)\n",
    "\n",
    "    print(f\"Cross-validation error for alpha = {alpha}: {np.mean(validation_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all cells are visible and have been run (rerun if necessary).\n",
    "\n",
    "The code below converts the ipynb file to PDF, and saves it to where this .ipynb file is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = # Enter here, the path to your notebook file, e.g. \"/content/drive/MyDrive/ECEN250/ECEN250_Lab6.ipynb\". Do not change the lines below, and make sure you do not have multiple notebooks with the same path.\n",
    "! pip install playwright\n",
    "! jupyter nbconvert --to webpdf --allow-chromium-download \"$NOTEBOOK_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download your notebook as an .ipynb file, then upload it along with the PDF file (saved in the same Google Drive folder as this notebook) to Canvas for Lab 4. Make sure that the PDF file matches your .ipynb file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rtRIzehBQ05c",
    "SOoGbCLaFyYm",
    "jYqRaCwxF5jg",
    "c4ua4E6J-woV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
