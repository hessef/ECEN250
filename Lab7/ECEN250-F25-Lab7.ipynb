{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDRvbLNKsL6O"
      },
      "source": [
        "#Part 1: Get vehicle mpg data into pandas and clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c06jO08Psa5O"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kby5Ug1ssWe9"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# Prompt for Authorization\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr-bfuK8sKQY"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/lab7/vehmpgdata.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZsoUmBxsZm2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/lab7/vehmpgdata.csv', encoding = 'unicode_escape')\n",
        "nRow, nCol = df.shape\n",
        "print(f'Dataframe has {nRow} rows {nCol} colums')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYyvDv_xtOA3"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY9o5Jm7tQ5W"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wqt3F1jvLd-"
      },
      "source": [
        "**Instruction: Since we will be using the datafile to predict the vehicle mpgclass, it would be cheating to use the city and highway mpg values, so drop the cmpg and hmpg features from your dataframe.Also eliminate the features that describe the type of vehicle, ‘sports', 'suv', 'wagon', 'minivan', 'pickup', ‘coupe’, and ‘sedan’.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da9RFJAyvMcG"
      },
      "outputs": [],
      "source": [
        "# drop the cmpg and hmpg features from your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi0FeflPvQpR"
      },
      "outputs": [],
      "source": [
        "# insert code to eliminate the features that describe the type of vehicle, ‘sports', 'suv', 'wagon', 'minivan', 'pickup', ‘coupe’, and ‘sedan’\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eK1VVsnwAr5"
      },
      "outputs": [],
      "source": [
        "# Creates a new dataframe with only the rows with NaNs in them\n",
        "df2=df[df.isnull().any(axis=1)]\n",
        "\n",
        "# Prints only the columns with NaNs\n",
        "print(df2.loc[:, df2.isnull().any()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2Fzc8Uv8P1"
      },
      "source": [
        "We will clean the dataframe to remove any NaNs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbJissntv9z6"
      },
      "outputs": [],
      "source": [
        "df=df.dropna(subset=['mpgclass'])\n",
        "df=df.dropna(subset=['weight'])\n",
        "df=df.dropna(subset=['wheelbase'])\n",
        "df=df.dropna(subset=['width'])\n",
        "df=df.dropna(subset=['cyl'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi2w9AWKwVZg"
      },
      "source": [
        "#Part 2: Train decision tree classifiers and evaluate their performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LecDsUT-wfBj"
      },
      "source": [
        "**With your now clean dataframe, select the remaining features and perform train/validation/test splitting of the dataset. Use 60%, 20%, 20% for your split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2tgteKdwaAl"
      },
      "outputs": [],
      "source": [
        "X=df.drop(columns=['mpgclass'])\n",
        "y=df[['mpgclass']]\n",
        "\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U4PUDAkwj9n"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# insert code to perform train/validation/test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApZfP8zIwp53"
      },
      "source": [
        "**Insert code below to print the size of the train/validation/test sets.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTOPbK-iyHYX"
      },
      "source": [
        "Now, we train a decision tree classifier using **gini** without any constraint on depth. Present the confusion matrix, accuracy, F1, and precision on **validation set** for that classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCv8fpVkyH3T"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
        "clf = clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-yIIYf_yMyo"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(x_validate)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3lEFSboyPr4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_validate, y_pred)\n",
        "print(confusion_matrix(y_validate, y_pred))\n",
        "\n",
        "# F1 & precision scores\n",
        "print(classification_report(y_validate, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJSjzyyNycGN"
      },
      "source": [
        "**Now, insert code below to train a decision tree using **entropy** without any constraint on depth. Present the confusion matrix, accuracy, F1, and precision for that classifier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhHLYAkdym8_"
      },
      "source": [
        "**Now, insert code below, using whichever criterion provided superior results, train decision trees with maximum_depth from 2 to 6. Present the confusion matrix, accuracy, F1, and precision for that classifier.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIX6GeIs0M2U"
      },
      "source": [
        "**Present the confusion matrix, accuracy, F1, and precision on your **test set** for the classifier which gave best results on validation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvIEMAySyp0Y"
      },
      "source": [
        "#Part 3: Train random forest classifiers and evaluate their performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQDzo1Zq0iL3"
      },
      "source": [
        "**We will use the same dataset to evaluate random forest classifiers. Using gini as the criterion and n_estimators=100, vary max_depth between 2 and 5. Select the best model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8smlTKHt0kbL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqWsoHvy0pXt"
      },
      "outputs": [],
      "source": [
        "depth = [2, 3, 4, 5]\n",
        "for i in depth:\n",
        "  # insert your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4LX6FPnF5FR"
      },
      "source": [
        "# Part 4: Data load, cleaning, and classification modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtVfkoeLuxFH"
      },
      "source": [
        "We will be analyzing heart ECG data to classify causes of arrythmia. This example is a classification of normal and abnormal ECGs based on 16 total classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu5CJUSUumuw"
      },
      "source": [
        "Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyAPKFIao7TT"
      },
      "source": [
        "The acutal number of instances for the measurements are given below.  We won't make use of this to set our priors -- but we will verify these classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcbHmm4lJ8Im"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Class Distribution:\n",
        "\n",
        "#  Class code :   Class   :                                             Number of instances:\n",
        "#    01             Normal                                                      245\n",
        "#    02             Ischemic changes (Coronary Artery Disease)                  44\n",
        "#    03             Old Anterior Myocardial Infarction                          15\n",
        "#    04             Old Inferior Myocardial Infarction                          15\n",
        "#    05             Sinus tachycardy                                            13\n",
        "#    06             Sinus bradycardy                                            25\n",
        "#    07             Ventricular Premature Contraction (PVC)                     3\n",
        "#    08             Supraventricular Premature Contraction                      2\n",
        "#    09             Left bundle branch block                                    9\n",
        "#    10             Right bundle branch block                                   50\n",
        "#    11             1. degree AtrioVentricular block                            0\n",
        "#    12             2. degree AV block                                          0\n",
        "#    13             3. degree AV block                                          0\n",
        "#    14             Left ventricule hypertrophy                                 4\n",
        "#    15             Atrial Fibrillation or Flutter                              5\n",
        "#    16             Others                                                      22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qzBbW4vKSE-"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFHT85H6vXW"
      },
      "source": [
        "Verify the datafile is in drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY9B7b3GX6AR"
      },
      "outputs": [],
      "source": [
        "!ls drive/MyDrive/lab7/data_arrhythmia.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jErE3uhG6yhJ"
      },
      "source": [
        "Load the datafile into pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I6yZSiDKZZm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('drive/MyDrive/lab7/data_arrhythmia.csv', delimiter=';')\n",
        "df.dataframeName = 'data_arrhythmia.csv'\n",
        "nRow, nCol = df.shape\n",
        "print(f'Dataframe has {nRow} rows {nCol} colums')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcdwhZSL665c"
      },
      "source": [
        "Notice this datafile has 280 features!  Much of the challenges in this lab is dealing with this large number of features.  The number of observations is actually quite low for this dataset.  It will be a challenge that will limit our classification effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRiS_7tnwY3w"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThMKL7-8wAjL"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OedIUYS9KdKQ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5XWGYgV7fbs"
      },
      "source": [
        "This piece of code is useful when you have such a large number of features.  It shows which features are objects -- which often means that it has non-numerics mixed with numerics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRfTd_ITY_ey"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['object'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbVPmkPJ6A6o"
      },
      "source": [
        "OH NO! This dataset has used the character ?  for a missing or unknown value.  There are three strategies for fixing this -- one would be directly dropping any entries with ?, second would be trying to replace each ? with something else, and the third is to first convert them to NaNs -- then use what we already know to fix the NaN problem.  Strategy 1 will throw away too much data, 2 is very difficult, so choose 3!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37d1uj5kNiWf"
      },
      "outputs": [],
      "source": [
        "#This will change object type to numeric and for non-numeric entries, force them to NaN\n",
        "df['P'] = pd.to_numeric(df['P'], errors='coerce')\n",
        "df['T'] = pd.to_numeric(df['T'], errors='coerce')\n",
        "df['QRST'] = pd.to_numeric(df['QRST'], errors='coerce')\n",
        "df['J'] = pd.to_numeric(df['J'], errors='coerce')\n",
        "df['heart_rate'] = pd.to_numeric(df['heart_rate'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m8-iYjh830w"
      },
      "source": [
        "**Now determine how many NaNs are left in P, T, QRST, J, and heart_rate. Run the code below to determine how many and include the results in a cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKPurxwU-J5_"
      },
      "outputs": [],
      "source": [
        "# The following code counts the number of valid numeric values in each column.\n",
        "featcount=df.count(numeric_only=True)\n",
        "featcount.sort_values()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9UAvtGMQdmH"
      },
      "source": [
        "**Question: how many NANs in columns P, T, QRST, J, and heart_rate?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dck6E1QQk1y"
      },
      "source": [
        "**Insert code below to fix the NaNs. You want to chose difference strategies for different columns.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M5CkBdwCa-W"
      },
      "source": [
        "\n",
        "\n",
        "**Make sure that there are no non numeric entries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O00hGWXeEAEZ"
      },
      "outputs": [],
      "source": [
        "df2=df[df.isnull().any(axis=1)]\n",
        "print(df2.loc[:, df2.isnull().any()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsxDKCNpYOKQ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNOrfTmRKmF-"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA9FQHI2quEi"
      },
      "source": [
        "We will now extract the label data: 'diagnosis' into y, and the remainder of the dataframe into x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIfHlhr4Kpyl"
      },
      "outputs": [],
      "source": [
        "y = df.diagnosis.values\n",
        "x = df.drop([\"diagnosis\"],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njn8tVLnrAFu"
      },
      "source": [
        "Next we need to partition our dataset in to Train, Validation, Test Sets:\n",
        "Insert code to  Split 20% of dataset to test, 20% to validation, and 60% into training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxkII_owKxia"
      },
      "outputs": [],
      "source": [
        "x_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgMsw61HK0z0"
      },
      "outputs": [],
      "source": [
        "x_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFsWRhx0tJUJ"
      },
      "source": [
        "Verify the data set partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-VT8iIKK4Wv"
      },
      "outputs": [],
      "source": [
        "print(\"x_train: \",x_train.shape)\n",
        "print(\"y_train: \",y_train.shape)\n",
        "print(\"x_test: \",x_test.shape)\n",
        "print(\"y_test: \",y_test.shape)\n",
        "print(\"x_validate: \",x_validate.shape)\n",
        "print(\"y_validate: \",y_validate.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ivAdA6itZ2h"
      },
      "source": [
        "Just a peek at the labels for the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fn8DYXaaop4"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo5gOAV1hPwL"
      },
      "source": [
        "We're going to use a random forest classiferto help us find he most important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJxpyMBmjYkr"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3x2xO0XhO4d"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(x_train, y_train)\n",
        "y_pred = rfc.predict(x_validate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aTV5bFBh9SY"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_validate, y_pred)\n",
        "print(\"Confusion Matrix: \")\n",
        "print(cm)\n",
        "print(\"Accuracy: \",accuracy_score(y_validate,y_pred))\n",
        "print(\"f1 Score: \",f1_score(y_validate, y_pred, average='macro'))\n",
        "print(\"Precision: \",precision_score(y_validate, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Y2g_Sijkw2"
      },
      "outputs": [],
      "source": [
        "# The following code shows the number of misclassified data in a confusion matrix\n",
        "misclassified = np.sum(cm) - np.sum(np.diag(cm))\n",
        "misclassified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mWJljQFeoZ"
      },
      "source": [
        "The following code uses the importance of the features in the Random Forest -- and sorts them from most important to least important and plots them in order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGuYzCyGiLJP"
      },
      "outputs": [],
      "source": [
        "importances = rfc.feature_importances_\n",
        "# Sort the feature importance in descending order\n",
        "sorted_indices = np.argsort(importances)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B03JqyhjiTMR"
      },
      "outputs": [],
      "source": [
        "plt.title('Feature Importance')\n",
        "plt.bar(range(x_train.shape[1]), importances[sorted_indices], align='center')\n",
        "plt.xticks(range(x_train.shape[1]), x.columns[sorted_indices], rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcTg39PfGZf3"
      },
      "source": [
        "Compute the most important 14 features -- which we will use later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKQPsriqcl0V"
      },
      "outputs": [],
      "source": [
        "importances[sorted_indices][0:14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2lli9SJcpuk"
      },
      "outputs": [],
      "source": [
        "x.columns[sorted_indices][0:14]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghtnKYW7GhH1"
      },
      "source": [
        "Now we are going to eliminate the least important 100 features from our analysis and retrain and evaluate our Random Forest classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNmSB6c4OMvR"
      },
      "outputs": [],
      "source": [
        "importances[sorted_indices][-100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8quG6JC-OlNN"
      },
      "outputs": [],
      "source": [
        "x.columns[sorted_indices][-100:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqXIkQK3GwAT"
      },
      "source": [
        "We now need to drop these features from our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ETM3-AkQ9WM"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = x.columns[sorted_indices][-100:]\n",
        "df.drop(columns_to_drop, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdiIz4N-S_Bq"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlqw6vgG5Tz"
      },
      "source": [
        "**Now, insert code to assign y to 'diagnosis' and the rest of the features to x; run train-test split for a 60% train, 20% validate, 20% test; train a Random Forest Classifier on train; generate CM, Accuracy, F1, and Precision for the validation set.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWFb9_XHcw0"
      },
      "source": [
        "**Now insert code to repeat the sorting of the feature importances; plotting of feature importance; finding of the next 100 least important features; and removal of those features from the dataframe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCTCIupxH3HB"
      },
      "source": [
        "**Now insert code to assign y and x as above; regenerate the train/validate/test datasets; instantiate and train a new random forest classifier; predict; generate CM, Accuracy, F1, and Precision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryDpWhu8IVsD"
      },
      "source": [
        "**Once more insert code to sort and plot feature importance, find the most important 14 features, and remove the rest from the dataframe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzNZnoY2IpmH"
      },
      "source": [
        "**Insert code to repeat the above to get a model that uses only the most important 14 features. Train, predict, and present results for that model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7PixTr_J0QH"
      },
      "source": [
        "**Now check the improtance of the features you used. Insert code to sort them and plot them**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD9rE-CYeaGU"
      },
      "source": [
        "**How does the sorted list of feature importance for these 14 compare with the original sorted list of features (before dropping the unimportant features)?  Open a text cell and give the original 14 and the current 14. Explain why these might be different.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQUP65ooPjDM"
      },
      "source": [
        "**Insert code to predict the **test results** for that model and present the CM, Accuracy, F1, and Precision of the final model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba0L3s2R16Y2"
      },
      "source": [
        "Before we get too proud of our ML skills, look at the last CM carefully.  The first row in the table is for normal ECG -- no arrythmia.  Our model performs well on these data.  For abnormal ECG, we seem to get the classifications correct about half the time, maybe a little better.  Modern ECG machines use very sophisticated signal analysis to detect conditions likely contributing to the abnormal ECG -- and cadiologists and nurses and technicians are trained to recognize the effect-cause relation shown in ECGs.  They are way better than our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4I-9Q6EtzL2"
      },
      "source": [
        "Lab 7 is now complete.  Make sure all cells are visible and have been run (rerun if necessary).\n",
        "\n",
        "The code below converts the ipynb file to PDF, and saves it to where this .ipynb file is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTEBOOK_PATH = # Enter here, the path to your notebook file, e.g. \"/content/drive/MyDrive/ECEN250/ECEN250_Lab7.ipynb\". Do not change the lines below, and make sure you do not have multiple notebooks with the same path.\n",
        "! pip install playwright\n",
        "! jupyter nbconvert --to webpdf --allow-chromium-download \"$NOTEBOOK_PATH\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download your notebook as an .ipynb file, then upload it along with the PDF file (saved in the same Google Drive folder as this notebook) to Canvas for Lab 7. Make sure that the PDF file matches your .ipynb file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
