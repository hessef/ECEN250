{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C8phZr22foN"
      },
      "source": [
        "\n",
        "## ECEN250 Lab 3 Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyaa9kjMakUC"
      },
      "source": [
        "**PART 1 Clustering:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAu16Fy1kLaQ"
      },
      "source": [
        "Clustering is an unsupervised machine learning technique used to group similar data points together. It helps in identifying patterns and structures within a dataset without predefined labels.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "1. Clusters – Groups of similar data points.\n",
        "\n",
        "2. Similarity – Measured using distance metrics like Euclidean distance.\n",
        "\n",
        "3. Unsupervised Learning – No prior labels; the algorithm finds patterns on its own.\n",
        "\n",
        "**Popular Clustering Algorithms:**\n",
        "\n",
        "1. K-Means – Partitions data into k clusters by minimizing the distance between points and their cluster centers.\n",
        "\n",
        "2. Hierarchical Clustering – Builds a tree of clusters, useful for understanding relationships.\n",
        "\n",
        "3. DBSCAN – Groups based on density, useful for irregularly shaped clusters.\n",
        "\n",
        "**Applications of Clustering:**\n",
        "\n",
        "1. Customer segmentation in marketing.\n",
        "\n",
        "2. Image compression and object recognition.\n",
        "\n",
        "3. Identifying patterns in biological data (e.g., gene expression analysis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES4Z3GZ82nmA"
      },
      "source": [
        "We will practice clustering in this lab first on synthetic cluster datasets.  We will then do clustering on our blower data.\n",
        "\n",
        "Start by importing necessary libraries -- including make_blobs that create the synthetic clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGsZzHSIa3fJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKyjgQPGa-B4"
      },
      "source": [
        "Create a synthetic cluster dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEIPBcFSbBhh"
      },
      "outputs": [],
      "source": [
        "blob_centers = np.array(\n",
        "    [[ 0.2,  2.3],\n",
        "     [-1.5 ,  2.3],\n",
        "     [-2.8,  1.8],\n",
        "     [-2.8,  2.8],\n",
        "     [-2.8,  1.3]])\n",
        "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvq615X1bDlU"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
        "                  cluster_std=blob_std, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onWCX9AsbIna"
      },
      "source": [
        "Plot the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A4ZlOQHbN_g"
      },
      "outputs": [],
      "source": [
        "def plot_clusters(X, y=None):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "GsF-GlXabSAt",
        "outputId": "4f70347e-2514-493b-ddcb-3281f85da7e6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_clusters(X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYAvOS_bdPa"
      },
      "source": [
        "Notice this uses scatter plots -- which we use extensively to visualize clustering in 2D. How many clusters do you see?  Now let's do k-means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANgeg41okwZY"
      },
      "source": [
        "**KMeans Clustering:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gJshTjgbWEu"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC-0szH23Uev"
      },
      "source": [
        "Now we are going to do k-means clustering.  This example is very obvious on the number of clusters that we should use.  Modify the following cell to specify the k value for this k-means example.  Once the k (which we call a hyperparameter) is specified, we use the method KMeans to specify that we will use the k-means clustering technique with n_clusters clusters.  In the next line, we use the method fit_predict of kmeans on our data X from above.  fit_predict first creates a model for the X dataset, then applies predict which returns the result of the model.  At times, we fit and predict in separate uses of those methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diL-zUQbbmeQ"
      },
      "outputs": [],
      "source": [
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVPd5_Z4brd5"
      },
      "source": [
        "We can use the kmeans attribute cluster_centers_ to show where the k-means model means or \"centroids\" were located when it completed the iterative assignment/mean movement process. Where are the centroids? List them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TGF8Klqbx9K",
        "outputId": "df011094-3c5a-4e27-bfdc-6cfd3b02ba71"
      },
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGPfevob6xc"
      },
      "source": [
        "We can now use this clustering to predict which cluster new observations belong. Notice that here we use the predict method to give us predictions for the 4 new points that we are interested in. The model returns the cluster number for which the mean is closest to the new point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u8H6Ahtb8Ty",
        "outputId": "86b7307b-ee4a-4b3a-8446-947acb0e0beb"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
        "kmeans.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhaS1euOcETc"
      },
      "source": [
        "This code will allows to look at the data together with the centroid (mean locations) and the boundaries that show which mean is closest to all of the areas in our 2D example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaErgYSPcFWv"
      },
      "outputs": [],
      "source": [
        "def plot_data(X):\n",
        "    #plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20, edgecolor='k')\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=24, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 0.1\n",
        "    maxs = X.max(axis=0) + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7YbBuC6jhh"
      },
      "source": [
        "Call this for the X data that we just clustered into the model kmeans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "FAjXGZ6YcQKh",
        "outputId": "8d73b09e-d5b1-4808-8828-d90c6c9ea9a8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-s-3_4FcVMt"
      },
      "source": [
        "Recall that kmeans is an iterative algorithm which starts with a (randomly) assigned location for centroids, assigns observed data to centroids, adjusts the centroid locations, and reassigns the cluster assignments to the new centroid locations, and repeats until no changes are required.\n",
        "\n",
        "Let's look at the first few iterations --- the following cell contains the code for the first iteration.  Notice that KMeans will set intial means locations, assign nearest, and compute the updated mean locations in 1 iteration.  You control the number of iterations that run through the max_iter parameter.  \n",
        "\n",
        "Modify the following cell to have three different models: kmeans_iter1, kmeans_iter2, and kmeans_iter3 for 1, 2, and 3 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "s4o6_jRdDDF5",
        "outputId": "7c9aa67c-8d8b-4afe-d1e3-04c29969a860"
      },
      "outputs": [],
      "source": [
        "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1, random_state=0)\n",
        "\n",
        "kmeans_iter1.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfIMGjXj7wOK"
      },
      "source": [
        "Now we can plot how our k-means improves as we add iterations.  Plot the three we just created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "amgMbJItcfap",
        "outputId": "beaa846e-d6b2-4d3d-e38e-9e23f0499f0e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.subplot(321)\n",
        "plot_data(X)\n",
        "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
        "plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
        "plt.tick_params(labelbottom=False)\n",
        "plt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n",
        "\n",
        "plt.subplot(322)\n",
        "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n",
        "plt.title(\"Assign the instances\", fontsize=14)\n",
        "\n",
        "plt.subplot(323)\n",
        "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n",
        "plot_centroids(kmeans_iter2.cluster_centers_)\n",
        "\n",
        "plt.subplot(324)\n",
        "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n",
        "\n",
        "plt.subplot(325)\n",
        "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
        "plot_centroids(kmeans_iter3.cluster_centers_)\n",
        "\n",
        "plt.subplot(326)\n",
        "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uclczdmclW2"
      },
      "source": [
        "Since the initial centroid assignment is done randomly, the same data can be clustered differently with different random initial assignments. The random assignment is controlled by the parameter random_state.  Lets first create some code to compare 2 clustering solutions side-by-side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbXYXhPhcmmM"
      },
      "outputs": [],
      "source": [
        "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
        "    clusterer1.fit(X)\n",
        "    clusterer2.fit(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 3.2))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plot_decision_boundaries(clusterer1, X)\n",
        "    if title1:\n",
        "        plt.title(title1, fontsize=14)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
        "    if title2:\n",
        "        plt.title(title2, fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARCVDiY68ZlL"
      },
      "source": [
        "Now let's create two  differnet clustering solutions for the same data (X) by instantiating two different models, each optimized from a different random initial mean location placement.  \n",
        "\n",
        "Modify the following cell for different random_states.\n",
        "\n",
        "Run the cell to see the difference.  \n",
        "\n",
        "Then try using the same random_state.  Are the solutions different?  \n",
        "\n",
        "Finally choose another set of different random states and rerun this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "JAnvLoCTcq7y",
        "outputId": "631da004-c89d-4cbe-d2c9-05a443b1251b"
      },
      "outputs": [],
      "source": [
        "#### insert code here\n",
        "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X, \"Solution 1\", \"Solution 2 (different random init)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTchhtklcyvO"
      },
      "source": [
        "To select the best model, we will need a way to evaluate a K-Mean model's performance. We can use inertia (the sum of the squared distances between each training instance and its closest centroid)as a metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggf97UlQc0dO",
        "outputId": "0b65bff8-a824-4816-c835-dc45b6040b25"
      },
      "outputs": [],
      "source": [
        "kmeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucia9Ctec7yJ"
      },
      "source": [
        "See how the two examples above have different intertia (make sure you reran the example with different random_state values!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhUvSd3Vc7Uc",
        "outputId": "e9fbe03b-01f1-4ebf-d6bc-3f8815801fc4"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init1.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF-_G27lc4GC",
        "outputId": "a25cbdef-4fc6-40c8-f0df-a596ed6519e8"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_init2.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX8L-0b0dGze"
      },
      "source": [
        "Because we have a way to compare our results with inertia, we can run the clustering multiple times (n_init times) and select the clustering with the minimum inertia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "yMk1IktAdIL8",
        "outputId": "00fb3721-a77e-4386-d148-3d88fb8f0443"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,random_state=2)\n",
        "kmeans_rnd_10_inits.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "sg6IWZckdP5w",
        "outputId": "4295a055-da1e-4839-e714-443e646d6dd1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvJtrS5D-ckx"
      },
      "source": [
        "Experiment by varying the number of initializations.  Remember that because these are random, with a single initialization you may get the best solution ... and sometimes it may take very many tries at the random initialization.  10 initializations is probably more than we need for this simple example, but for more complex datasets it can require that many, or more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDXuS5DpdXFi"
      },
      "source": [
        "So, we can find best clustering for a given k, but how do we find the best number of clusters?\n",
        "\n",
        "We've tried 5. Let's try a smaller and larger k.\n",
        "\n",
        "Add a cell after this cell that creates two kmeans models -- one for k=3 and another for k=8.  \n",
        "\n",
        "Then use the plot_clusterer_comparison() function defined above to compare the two results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "bw42uQkrdYQw",
        "outputId": "87f09784-bfe1-4ec4-fef2-ec694bfb1a0c"
      },
      "outputs": [],
      "source": [
        "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
        "\n",
        "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8b5z2bmEH83"
      },
      "source": [
        "Compare these results to the k=5 example from above. Which k seems to fit our dataset best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kldvksjdgEz"
      },
      "source": [
        "Remember that we have a way to compare our k_means models:  inertia.  Insert cells below this cell to print the inertia from your k=3 and your k=8 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GoLw3trAUQu"
      },
      "source": [
        "If we used minimum inertia as our metric, which model gives the best results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo2ZI9TzE05q"
      },
      "source": [
        "ANS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7mLkOMpAgS2"
      },
      "source": [
        "Is that consistent with the plots that we have just looked at?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62i862S2xVka"
      },
      "source": [
        "ANS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbRQRc_Hdsrx"
      },
      "source": [
        "So, inertia alone isn't what we need, since it decreases with increasing k [This should be obvious, right?! -- since it is the sum of squared distance to the nearest centroid). More centroids means we can place them close to our data.  What would our inertia be if we use 2000-means on our X data?  Recall X is made up of 2000 data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdNPNOGXBTq2"
      },
      "source": [
        "A better metric is to see how inertia changes as we consider additional clusters.  The following code computes inertia for k=1 to k=10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-8kTWtudtzd"
      },
      "outputs": [],
      "source": [
        "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "                for k in range(1, 10)]\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdNmfgXfBoR4"
      },
      "source": [
        "By plotting this data, we can see where adding additional means (adding one to k) begins to help less and less.  That point is called the elbow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "MyWL9T1ndyKm",
        "outputId": "344955a9-b5e6-432c-8225-3032349d69a6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(range(1, 10), inertias, \"bo-\")\n",
        "plt.xlabel(\"$k$\", fontsize=14)\n",
        "plt.ylabel(\"Inertia\", fontsize=14)\n",
        "plt.annotate('Elbow',\n",
        "             xy=(4, inertias[3]),\n",
        "             xytext=(0.55, 0.55),\n",
        "             textcoords='figure fraction',\n",
        "             fontsize=16,\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
        "            )\n",
        "plt.axis([1, 8.5, 0, 1300])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXunT0xd--Z"
      },
      "source": [
        "So, we can look at changes in inertia as a selection criteria for k -- we should be using k values at or above the elbow in the trend. In this example, k=4 does not quite accuractly capture the clustering characteristics, but 5 does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "kWRU3r2deACS",
        "outputId": "9e41b196-5199-4019-fca3-648244a3bbce"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbq9GsHcCNKi"
      },
      "source": [
        "Let's look at where k-means struggles -- where clusters are elongated or intertwined. Here the metric of sum of squared distance to the centroid leads to too simple modeling.  First let's create a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQlF_XhVeJRq"
      },
      "outputs": [],
      "source": [
        "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
        "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
        "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
        "X2 = X2 + [6, -8]\n",
        "X = np.r_[X1, X2]\n",
        "y = np.r_[y1, y2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoFu26aMeKr6"
      },
      "source": [
        "Insert a cell with a call to our clustering plotter for this new dataset X. Like we did above for the simple 5 cluster blobs set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg5HKoyCCnqA"
      },
      "source": [
        "We are going to let KMeans try to find good solution with a random initialization, and with an initialization that we give to the clusterer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "uz4v6rvleWbb",
        "outputId": "6a03c78a-15fc-458f-e9ae-ea7fa39e8fae"
      },
      "outputs": [],
      "source": [
        "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=123)\n",
        "kmeans_bad = KMeans(n_clusters=3, random_state=123)\n",
        "kmeans_good.fit(X)\n",
        "kmeans_bad.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhoLNTicC_cu"
      },
      "source": [
        "Lets compare the two -- visually and by computing inertias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "U9220hI5eeJJ",
        "outputId": "5ce817ec-44c0-4cbc-a895-3afa295958c2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 3.2))\n",
        "\n",
        "plt.subplot(121)\n",
        "plot_decision_boundaries(kmeans_good, X)\n",
        "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
        "\n",
        "plt.subplot(122)\n",
        "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
        "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hL57j2cDiMB"
      },
      "source": [
        "Notice, again that inertia isn't always a measure of the best clustering solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEmNpzSAei9Y"
      },
      "source": [
        "Scikit-learn has many other clustering models that more accurately handle cases like this for which k-means is not the best model!\n",
        "\n",
        "Choose a clustering method that seems more appropriate for the clusters we are using.\n",
        "\n",
        "Insert a cell below that uses your improved clustering method our X data with a better clustering method. [NOTE you will probably need to import that clusterer before you use it!]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLTplJbBOya6"
      },
      "source": [
        "Since most of the alternative clustering methods are not implemented with predict() methods, showing decision boundaries like we have done for k-means isn't practical.  Instead let's just look at the assignment of the X points to clusters by coloring the dataset by class.  You need to modify the following cell to match the model that you created in the above cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "pR5N6EF0NSRV",
        "outputId": "256ff9f2-685d-470e-df85-aec3783e6ef8"
      },
      "outputs": [],
      "source": [
        "# Visualize it:\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:,0], X[:,1], c=ImprovedCluster.labels_.astype(float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnGS_DXYiI9C"
      },
      "source": [
        "# Part 2: Statistics on Blower Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqcaeIcxvKJf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbdmLnaGvpef"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXYtyNuEvzrr"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-4btK1Ea4f3"
      },
      "source": [
        "In the previous lab, you worked on cleaning a CSV file. For this lab, we provide the cleaned version in your lab folder, and you will use it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUmXZvyYwEXU"
      },
      "outputs": [],
      "source": [
        "!ls drive/MyDrive/ECEN250LeafBlowersClean.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w4r-WMdwOEO"
      },
      "outputs": [],
      "source": [
        "# importing dataset\n",
        "df = pd.read_csv('drive/MyDrive/ECEN250LeafBlowersClean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyD5YBbXwXrH"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaJh01hSbODH"
      },
      "source": [
        "Insert code to drop any non-numeric field in the CSV!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp5ZvXqLK8ge"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scs_rAoF4IMU"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0p5CNnnrFKf"
      },
      "source": [
        "Now that we have reloaded our clean blower data, lets look at some statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W4GokXbrvNc"
      },
      "outputs": [],
      "source": [
        "df['price'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imlz3uWbbuPM"
      },
      "source": [
        "Insert a cell to give the median of the price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjvp8lfqb5vw"
      },
      "source": [
        "Let's do the standard deviations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7KebpZtsFTa"
      },
      "outputs": [],
      "source": [
        "df['price'].std(ddof=0) # this is for the population standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSqh1tAHsLe_"
      },
      "outputs": [],
      "source": [
        "df['price'].std(ddof=1) # we use this if we're doing sample standard deviation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6aHFTgdtVLg"
      },
      "source": [
        "Recall we can get a summary of statistics for the full dataframe using the Pandas describe() method. Insert a cell and do the describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsy0B9xClmq4"
      },
      "source": [
        "What type of standard deviation is the default for pandas statistics?  Can you explain why they made that choice?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN0dUBB7ty9G"
      },
      "source": [
        "We can also look at statistics for subsets of our blowers.  For instance we can look a differences in the mean price for entries with zero, one, and two batteries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGbu9MWQsi-q"
      },
      "outputs": [],
      "source": [
        "nobat=df.loc[df['no batteries'] == 0]\n",
        "onebat=df.loc[df['no batteries'] == 1]\n",
        "twobat=df.loc[df['no batteries'] == 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv7amuhBsocm"
      },
      "outputs": [],
      "source": [
        "nobat['price'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtX0aS-qsuXn"
      },
      "outputs": [],
      "source": [
        "onebat['price'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ECAEiUsxs-"
      },
      "outputs": [],
      "source": [
        "twobat['price'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJa9q3M8eyQe"
      },
      "source": [
        "Open cells following this cell and compare the difference in mean between zero and 1 battery and the difference in mean price between 1 and 2 batteries. Explicitly write code to compute the mean differences. Should these differences be the same?  Why might they be different?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAmdDkqSuF_E"
      },
      "source": [
        "Histograms help us see how our data is distributed -- and help us visualize where our sampling plans may be limiting our analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIstAwv_ncnv"
      },
      "outputs": [],
      "source": [
        "df['no batteries'].plot.hist(grid=True, bins=8, rwidth=0.9,\n",
        "                   color='#0504aa')\n",
        "pyplot.title(\"Number of batteries\")\n",
        "pyplot.ylabel('Frequency')\n",
        "pyplot.xlabel('No batteries')\n",
        "pyplot.grid(axis='y', alpha=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2hKK0XIs2TW"
      },
      "outputs": [],
      "source": [
        "df['volt'].plot.hist(grid=True, bins=8, rwidth=0.9,\n",
        "                   color='#0504aa')\n",
        "pyplot.title('Blower Voltage')\n",
        "pyplot.ylabel('Frequency')\n",
        "pyplot.xlabel('Blower Voltage')\n",
        "pyplot.grid(axis='y', alpha=0.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIZG7lZwc41x"
      },
      "source": [
        "Insert a code cell which creates an 8-bin histogram for the hi cfm feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtrwYCw1dI2s"
      },
      "source": [
        "Now insert an 8-bin histogram for price.  Put labels on the x and y axes which describe what the axes represents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrYdjbgthvPM"
      },
      "source": [
        "As we saw above describe() gives us lots of statistics about our features.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc1OZDRhvpf3"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz8sa4uOobfx"
      },
      "source": [
        "Use a boxplot (you used boxplot in Lab 2 -- look back if you can't remember the syntax for the method) to look at how the prices are distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WMmLuIpHT16"
      },
      "source": [
        "Do you have blowers with prices outside the wiskers?  What should we do with these -- keep them in our analysis? exclude them?  Justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE9SrJcr5Rll"
      },
      "source": [
        "Our anaysis gets more complicated and more revealing if we look at multiple features and the relationships between these features.  Let's start with a\n",
        "scatter plot for performance -- hi cfm vs hi mph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EhKG-XKAtPo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gri_LMj5Qk0"
      },
      "outputs": [],
      "source": [
        "# Plot outputs\n",
        "pyplot.scatter(df['hi cfm'], df['hi mph'], color=\"green\", label=\"Test\")\n",
        "\n",
        "\n",
        "pyplot.xlabel(\"hi cfm\")\n",
        "pyplot.ylabel(\"hi mph\")\n",
        "#pyplot.xlim((-1, 8))\n",
        "#pyplot.ylim((0, 18))\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPLEpt28pzDE"
      },
      "source": [
        "Now look at hi mph vs price. Insert a cell to show the cluster plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQDJjFAxyPwb"
      },
      "source": [
        "Looking at weight statistics may help you find problem entries.  For example: look at no. batteries vs weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNeF8cniOOck"
      },
      "outputs": [],
      "source": [
        "# Plot outputs\n",
        "pyplot.scatter(df['no batteries'], df['weight'], color=\"green\", label=\"Test\")\n",
        "\n",
        "\n",
        "pyplot.xlabel(\"no batteries\")\n",
        "pyplot.ylabel(\"wieght\")\n",
        "#pyplot.xlim((-1, 8))\n",
        "#pyplot.ylim((0, 18))\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJr7ac31ylMB"
      },
      "source": [
        "Or price -- it might also help to do this for only one voltage at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wj-x0XAOqwc"
      },
      "outputs": [],
      "source": [
        "# Plot outputs\n",
        "pyplot.scatter(df['no batteries'], df['price'], color=\"green\", label=\"Test\")\n",
        "\n",
        "\n",
        "pyplot.xlabel(\"no batteries\")\n",
        "pyplot.ylabel(\"price\")\n",
        "#pyplot.xlim((-1, 8))\n",
        "#pyplot.ylim((0, 18))\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyrazNw7sQJ_"
      },
      "source": [
        "NOW: we can look at how different features in our dataframe are related to each other.  Recall that covariance give a measure of relation between the variances of two random variables (features).  We can compute covariances using df.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqq8BWp0zOrQ"
      },
      "outputs": [],
      "source": [
        "df.cov()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEI4r8dqzarE"
      },
      "source": [
        "More often we examine Pearson's correlation coefficient, R, to determine how features are correlated. This is done with df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lgjxqP0zZQ-"
      },
      "outputs": [],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ty3pEw0BBE"
      },
      "source": [
        "This table tells us about important relationsips between features.  If we start with price, we see that price depends highly on the cfm  (which is a key performance characteristic!).  Also the price depends on the number of batteries included in the price. Weight is highly correlated with price. We see that with brushless=0 and brushed=1, that price is negatively correlated to price.  Brushed motor blowers are generally cheaper than brushless.  \n",
        "\n",
        "To help us visualize where we may have issues with our data, we can use pair-plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpjIWoEMpjyj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv5NIeJstx_B"
      },
      "source": [
        "Using Seaborn pair-plotting, we can observe visually the correlations and the histogram distributions of the features.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iat-gmAht6vd"
      },
      "outputs": [],
      "source": [
        "df_plot = df.iloc[:, 3:15]\n",
        "sns.pairplot(df_plot, diag_kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHXu2hgVkN72"
      },
      "source": [
        "Note that the histograms on the diagonals help show us how the data in our features and distributed and where there are gaps.  Notice that highly correlated blobs of points in the pair-plot off-diagonal figures will be stretched out from lower left to upper right.  Uncorrelated will be blobs with rising or falling trends. Negatively correlated features will have pair plots that stretch from upper left to lower right -- look at price v. motor type above for an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSNaTlhl2VSW"
      },
      "source": [
        "Given that price is highly correlated with high cfm, repeat the above analysis that included the scatter plot for the subset of data hi cfm and price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3OFRcMUQqlt"
      },
      "source": [
        "# Part 3: Clustering with your blower data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkshuv6Xdy4A"
      },
      "source": [
        "For the data in the above scatter plot, let's use our k-means clustering technique to cluster that data -- let's try 3 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pnXUG2MRTps"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=1953)\n",
        "X=df[['hi cfm','price']].to_numpy()\n",
        "y_pred = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13QQ89apRppi"
      },
      "outputs": [],
      "source": [
        "# Visualize it:\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_.astype(float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYuM9l2keEdo"
      },
      "source": [
        "Insert a cell below to do k-means clustering of hi cfm and hi mph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Uw_fZKeMdM"
      },
      "source": [
        "Plot it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVsLJxDZ24lp"
      },
      "outputs": [],
      "source": [
        "def plot_data(X):\n",
        "    #plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20, edgecolor='k')\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=24, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 100\n",
        "    maxs = X.max(axis=0) + 100\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tROBoAu3fqy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-s7ngfal42a"
      },
      "source": [
        "Go back 3 code cells and modify the line k=3 and rerun these cells.  Try a few values of k. Which seems to most closely represent the data in the scatter plot?  Recall we have the elbow method if we wanted to more fully analyze this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkTk04WR7Rcf"
      },
      "source": [
        "Repeat cluster analysis for price vs number of batteries included in the price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZsodfbYebiP"
      },
      "source": [
        "Add the code cell to build the k-means clustering for k=3 for these two features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9juu-I6ekwN"
      },
      "source": [
        "Again plot it, showing decision boundaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLzevEbJnV6u"
      },
      "outputs": [],
      "source": [
        "def plot_data(X):\n",
        "    #plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20, edgecolor='k')\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=24, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 1\n",
        "    maxs = X.max(axis=0) + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO0UwNErnjzE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ2pomtZnsyp"
      },
      "source": [
        "Does this look like reasonable clusters for the scatter diagram that we just saw?  You may think that there is a problem with the python code.....perhaps, but it is more likely that we have an issue with the data that we are trying to cluster.  Recall that clustering uses Euclidian distances in determining similarity for forming clusers. Open a cell following this and rerun df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPHwqQYVox8O"
      },
      "source": [
        "Open a text cell following this cell and give the range of values for price and for no batteries.  Describe generally (no equations) how these difference will affect distances in the 2D plane for price vs no batteries. Suggest a technique that may improve this clustering analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYCdqT0ErcyI"
      },
      "source": [
        "In the cells below, create a new, scaled dataframe and use the scaled dataframe to repeat the price, no batteries clustering and k-means clustering from the last several cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnTQmAEDx02W"
      },
      "outputs": [],
      "source": [
        "# Plot outputs\n",
        "pyplot.scatter(standardized_df['no batteries'], standardized_df['price'], color=\"green\")\n",
        "\n",
        "\n",
        "pyplot.ylabel(\"price\")\n",
        "pyplot.xlabel(\"no batteries\")\n",
        "#pyplot.xlim((-1, 8))\n",
        "#pyplot.ylim((0, 18))\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF1coa8aXLK6"
      },
      "source": [
        "Notice that for numeric categorical features like number of batteries or voltage that even scaling for k-means doesn't give us decision boundaries that we might expect.  These might do better with an alternative clustering method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmHCZKDVzKwV"
      },
      "source": [
        "This completes Lab 3.  Now make sure that this notebook shows all your executed cells. If it does not, you can restart the runtime and runall or you can restart the runtime and individually reexecute all of your cells.\n",
        "\n",
        "The code below converts the ipynb file to PDF, and saves it to where this .ipynb file is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTEBOOK_PATH = # Enter here, the path to your notebook file, e.g. \"/content/drive/MyDrive/ECEN250/ECEN250_Lab3.ipynb\"\n",
        "! pip install playwright\n",
        "! jupyter nbconvert --to webpdf --allow-chromium-download \"$NOTEBOOK_PATH\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download your notebook as an .ipynb file, then upload it along with the PDF file (saved in the same Google Drive folder as this notebook) to Canvas for Lab 3. Make sure that the PDF file matches your .ipynb file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
