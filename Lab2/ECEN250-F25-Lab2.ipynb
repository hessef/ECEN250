{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKxJcnqzpTAO"
      },
      "source": [
        "PART 1: Moving data into a dataframe and manipulating it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk5_e0ipTX0c"
      },
      "source": [
        "First, import libraries that we will use in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VWJm2OBchfhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sKg1RVw0hvfG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwYYLOuPTlLE"
      },
      "source": [
        "Now, we want to mount Google Drive to Colab to be able to read data from the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9HTJ_DGph2Mi",
        "outputId": "6fe7cbde-39ea-4286-ec25-7cbe960ca278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJ1wWkET7Jf"
      },
      "source": [
        "Use unix ls command to make sure that the file we will load is on MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8d-JuK8lbo5"
      },
      "outputs": [],
      "source": [
        "!ls drive/MyDrive/CstatWeatherNov19.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1ESPU4eUIZp"
      },
      "source": [
        "Insert a code cell below this one that contains code to import the csv data from the file into a dataframe df using Pandas read_csv  -- note that if you use interational charactersets, sometimes these can fail during read_csv().  Often including encoding = 'unicode_escape' in the read_csv() fixes this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2oJWeh1UWZx"
      },
      "source": [
        "Use info() to look at the number and types of data that we loaded into the dataframe. Here we use the \"dot notation.\" Dot notation allows us to refer to method of the object (method is a function exclusive to a specific class/object/instance) or attribute of the instance/object.  Methods have () at the end of the name, attributes do not.  So here's the info() method for the dataframe df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCbg-afDmFUg"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TfMERiUkBB"
      },
      "source": [
        "Python machine learning tools that we will use extensively operate on numeric data.  We have Day which is an object type and Precip inch (rainfall data) is currently an object type.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIg-OvXEUj8F"
      },
      "source": [
        "Let's look at the first few rows of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnYdAg2VwfKD"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaPWUAtmF0z0"
      },
      "source": [
        "First, convert Day into a special type which will allow us to do date/time manipulations using Pandas to_datefime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTB6hWqLtucO"
      },
      "outputs": [],
      "source": [
        "df['Day'] = pd.to_datetime(df['Day'], format = '%m/%d/%Y')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6oauK9NVe20"
      },
      "source": [
        "Now insert a cell to check the info() again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkiKwefbVpjC"
      },
      "source": [
        "Next, Precip inch.  Look at the values for precipitation to see why they are objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBy1GtyuV6Kx"
      },
      "outputs": [],
      "source": [
        "df['Precip inch']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loa3cV4oWNOb"
      },
      "source": [
        "The use of T for trace rainfall is causing the problem. Trace means that detectible precipitation of less than 0.005 inches were detected.  For simplicity, substitute a small number for T. Note: trace in snow depth is defined as less than 0.05 inches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pelj0gGPs0AY"
      },
      "outputs": [],
      "source": [
        "df.loc[df['Precip inch']=='T', ['Precip inch']] = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsv_TlXtXv5l"
      },
      "source": [
        "Now, we've eliminated the Ts, but that doesn't automatically change the Dtype.  Change the Dtype to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-5J7hCOnZXq"
      },
      "outputs": [],
      "source": [
        "df['Precip inch'] = df['Precip inch'].astype(float, errors = 'raise')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9H0F6PFYVGC"
      },
      "source": [
        "Again, insert a cell and use info() to check the type is now numeric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GcKORHAYfm6"
      },
      "source": [
        "So, look at all the cleaned data. Since this is a small dataframe we can look at it all at once.  For large datasets, use df.head() to show the first 5 rows. df.tail(7) will show the last 7 rows in the dataframe.  Experiement with these in the next few cells.  Try the entire dataframe, then the first 5 rows and the last 7 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3juK4Ivdz-q1"
      },
      "outputs": [],
      "source": [
        "#df\n",
        "#df.head()\n",
        "df.tail(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwVLRMbTZqqn"
      },
      "source": [
        "This dataframe is quite clean.  We could do some additional things like fixing consistency of capitalization of the feature names. df.rename() would change the column names.\n",
        "\n",
        "Instead, lets work on a dirty data example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f6cYjnRcDCa"
      },
      "source": [
        "This file DataClean.csv is on canvas. Insert some cells that: Go get the file, make sure it is in your MyDrive directory, and read it into a dataframe named dirtydf (following what we did above for the  weather data.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7QCBjOvHDLB"
      },
      "outputs": [],
      "source": [
        "#see what it looks like\n",
        "dirtydf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zEcAC1ZaHIZ"
      },
      "source": [
        "This dataframe is a MESS.  Each feature (column) should have 9 observations or instances (rows). Let's take a look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhIH1TwKaF0i"
      },
      "outputs": [],
      "source": [
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOan6pgceCpB"
      },
      "source": [
        "We can use the dataframe count method to see how many values we have in our datrframe.  NaN values are not counted in count(). Many are obviously missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhmfuCSZdc0G"
      },
      "outputs": [],
      "source": [
        "dirtydf.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEQITlNgvpMO"
      },
      "source": [
        "First, let's fix the Date object like we did before. Insert cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR6kHIKyvvbS"
      },
      "outputs": [],
      "source": [
        "dirtydf['Date'] = pd.to_datetime(dirtydf['Date'], format = '%m/%d/%Y')\n",
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLBC0_wzeTQ7"
      },
      "source": [
        "Since we have only one reading on Gage 4, doing statistical analysis will not be very useful for this column.  Let's just drop this entire column from our dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE1Io3xyi4Su"
      },
      "outputs": [],
      "source": [
        "dirtydf = dirtydf.drop(['Gage 4'], axis=1) #many ways of specifying how to drop this column. Check out pandas documentation for others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6cd-4Zubb6N"
      },
      "source": [
        "How does it look now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMg-jQqmaBrP"
      },
      "outputs": [],
      "source": [
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou8gY9OMlXZ0"
      },
      "source": [
        "Let's next drop the row from the day that Dr Lowe was sick and didn't take any readings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDGGZDIplnBk"
      },
      "outputs": [],
      "source": [
        "dirtydf.drop([4], inplace=True) # drops index 4 (row 4) within same dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vV82sC0odPN"
      },
      "outputs": [],
      "source": [
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_GTBMsdjehM"
      },
      "source": [
        "Now, let's fix those NaNs. Pandas dropna() and fillna() are very flexible. We can force the NA to a value (eg 0) or the min/max/mean/etc of that column. We can also replace it with the last valid entry. Let's try that:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ph_iWMtjlFU"
      },
      "outputs": [],
      "source": [
        "dirtydf['Gage 2'].fillna( method ='ffill', inplace = True) # use forward fill method\n",
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmBIXqvSuaLg"
      },
      "source": [
        "Lets look at info() again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8rv707s3FHX"
      },
      "outputs": [],
      "source": [
        "dirtydf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_Ai7y43SRm"
      },
      "source": [
        "We want all our fields to be numeric so that we can generate statistics.  Fix the Dtype of Gage 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vShCs4f3djq"
      },
      "outputs": [],
      "source": [
        "dirtydf['Gage 1'] = pd.to_numeric(dirtydf['Gage 1'])\n",
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIDxxKalpBB0"
      },
      "source": [
        "For reasons that will become clearer when we do ML classification, we want to have the Measurer be categorical Dtype rather than Object. We can accomplish this with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWeYddkMpH7Y"
      },
      "outputs": [],
      "source": [
        "dirtydf[\"Measurer\"] = dirtydf[\"Measurer\"].astype(\"category\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TOvkDndpVu5"
      },
      "source": [
        "Let's take another look at the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XTjZNNSpaol"
      },
      "outputs": [],
      "source": [
        "dirtydf.info()\n",
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDBpC0SSpm9y"
      },
      "source": [
        "Later, we may turn the categorical feature into a numeric value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEp_9Lv7jzXI"
      },
      "source": [
        "Now, what are we going to do about the obviously erroneous reading of Gage 2 on Oct 6?\n",
        "\n",
        "It is very obvious this is a outlier due to some error.Now let's look at that entry for Gage 2 which seems to be an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-OoSS5CkQOG"
      },
      "source": [
        "We will generally just use visualization techniques for this class to help us recognize outliers.  This example will be very obvious, but let's go through a set of techniques that can help with datasets with many more observations and with outliers which are not so obvious.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9k_A-ODksHD"
      },
      "source": [
        "First, look at a sort of the data values for the Gage 2 feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8q5F5Spk4Fu"
      },
      "outputs": [],
      "source": [
        "dirtydf['Gage 2'].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hz835MclC4Q"
      },
      "source": [
        "Note if there were hundreds or thousands of observations, we'd need to look at head() or tail().  Practice here by inserting a cell to print the first 3 and the last 3 entries of the sorted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7sgOf6Dq3tr"
      },
      "source": [
        "Sorted features make it easier to see what may be abnormally small or large.  Another visualization technique is boxplot. Boxplots show the distribution of the data for a row or column with a box for the data from the 1st to the 3rd quartile and \"wiskers\" for data 1.5X the extent of the box.  For many features, data values outside the wiskers can be considered outliers. Be careful, however, with features where the data is exponential in scale. But our values are not exponential, so let's use box plot \"wiskers\" to guide us.  Let's start by generating a boxplot for Gage 3, which seems to have no outliers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEcVilvxrDye"
      },
      "outputs": [],
      "source": [
        "boxplot = dirtydf.boxplot(column=['Gage 3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1WfFcQsfJu"
      },
      "source": [
        "Now repeat this for Gage 2: Insert a cell to create a boxplot for Gage 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJCdTvBtcKw"
      },
      "source": [
        "Now this value is obviously an error. The outlier is way outside the box and wiskers, which are compressed into a single green line in this plot.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZayiRkBUm9y"
      },
      "source": [
        "We can (1) try to fix the error if we can determine what a likely correct value is, (2) drop the feature or the observation using dirtydf.drop(), or (3) we can replace outliers with another value 0, min, max, mean, ...\n",
        "\n",
        "By examination it seems like two measurements, each without their decimal point were incorrectly entered for this datum.  Let's assume that the first four digits with the decimal point after the first digit is the correct reading for Gage 2 on Oct 6.\n",
        "\n",
        "We could try to fix that by doing a replacement: dirtydf.at[index, column] = dirtydf.at[index, column] / 1000000, with the correct index and column label\n",
        "\n",
        "However, that leaves the erroneous 4 digits at the end of the number.  An alternative would be to first do modular division on the incorrect datum to clear the last 4 digits followed by a real division to adjust the decimal point.\n",
        "\n",
        "Insert cells to correct the datum at index 5 of feature Gage 2 as described."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PtLh0kBzvrJ"
      },
      "source": [
        "Show your fixed dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDZD-KXLzt2A"
      },
      "outputs": [],
      "source": [
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyfMqJ1MYXio"
      },
      "source": [
        "OK, so now that it is clean let's do some data frame manpulations:\n",
        "Let's add an additional column (attribute + data = feature) that is the average of the readings of Gage 1 and Gage 2.  We will use a new series to compute the average then we will insert thate as a new column in the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oKRv38NSXhW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDpJrk6R77A_"
      },
      "outputs": [],
      "source": [
        "avgG1G2=dirtydf[[\"Gage 1\", \"Gage 2\"]].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4E60QA3RcYx"
      },
      "outputs": [],
      "source": [
        "avgG1G2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0SSFk_wCluw"
      },
      "source": [
        "We can use several techniques to insert this numpy series back into our dataframe df.insert(position, name, series to insert) works well for this.  df.assign() will also work.  Open a cell and use Pandas dataframe .insert() to add avgG1G2 into the dataframe dirtydf at position 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdUdeFM8EVpK"
      },
      "source": [
        "now show the dataframe to verify that the new feature has been added to the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMx5yzxwRYcX"
      },
      "outputs": [],
      "source": [
        "dirtydf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNWbpxAmpnhw"
      },
      "source": [
        "Let's look at subsets of our dataset.  Let's just look at the data Measured by Abe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hoq1mOTau1OO"
      },
      "outputs": [],
      "source": [
        "dirtydf['Measurer']=='Abe'# which indices correspond to Abe's measurements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr4nwHYju22r"
      },
      "outputs": [],
      "source": [
        "dirtydf[dirtydf['Measurer']=='Abe']# subset of dataset from Abe's measurements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfM5LpDvp1uh"
      },
      "outputs": [],
      "source": [
        "dirtydf.loc[dirtydf['Measurer']=='Abe',['Gage 3']]# the readings from Gage 3 that were measured by Abe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaRtjyydWajW"
      },
      "source": [
        "Being able to subset our observations by a value (or range of values) in one feature is important.  Equally important is being able to subset the corresponding data from another feature for the subset determined by another feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QamrtSSW5Rc"
      },
      "source": [
        "Remember back to our eliminating the observation that had missing data since Dr. Lowe was out sick.  In that case we used index 4 to eliminate that observation with df.drop().\n",
        "If Dr. Lowe turned out to consistently be a bad Gage reader, we could use the above subsetting technique to select only observations made by other observers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d-Pnbs173OB"
      },
      "source": [
        "OK, so now back to our weather data in the dataframe df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baKwF8CubmF3"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4wQ-wgb6XO"
      },
      "source": [
        "Count includes numeric values (not a number entries or NANs are not counted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGCfIs2ucN7_"
      },
      "source": [
        "Compute the Proportion of observations with precipitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psAf8xK5dCRb"
      },
      "outputs": [],
      "source": [
        "((df['Precip inch'] != 0).sum()/df['Precip inch'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ3tkAwje_dJ"
      },
      "source": [
        "We could do other, more complex, Proportion calculations. Proportion of days with more than a Trace of rain.  For Percent, add a multiplication by 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3mfp5hfr_I"
      },
      "source": [
        "Histograms are a good way to look at Frequency Distribution.  \n",
        "First, use Matplotlib to create a simple histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rONqaqsJrTT4"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.hist(column='high degree F')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8JDHJiAgmIt"
      },
      "source": [
        "Adding axis labels, title, gridlines, limits, etc using Matplotlib a better looking histogram results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM3KugZdudnv"
      },
      "outputs": [],
      "source": [
        "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
        "n, bins, patches = pyplot.hist(x=df['high degree F'], bins='auto', color='#0504aa',\n",
        "                            alpha=0.7, rwidth=0.85)\n",
        "pyplot.grid(axis='y', alpha=0.75)\n",
        "pyplot.xlabel('high temp (F)')\n",
        "pyplot.ylabel('Frequency')\n",
        "pyplot.title('Nov 2019 College Station daily high temperature')\n",
        "maxfreq = n.max()\n",
        "# Set a clean upper y-axis limit.\n",
        "pyplot.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015F6WvMhjVx"
      },
      "source": [
        "If we want more bins for the histogram adjust the number of bins (6 to 16 is considered best practice). Insert the code after this cell to creat a 16 bin histogram of the daily high temperature:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb7THfq5iOny"
      },
      "source": [
        "Measures of Central Tendancy\n",
        "\n",
        "Mean, median, mode can all be computed on the dataframe columns. Pandas has a method for mean(), median(), and mode() for datframes.  Let's find the means of the daily high temperature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4mzifv5IPxm"
      },
      "outputs": [],
      "source": [
        "print(\"mean:\", df['high degree F'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61oS0PLEI1K0"
      },
      "source": [
        "OR we could find the mean of all features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXYoh4TLJAHx"
      },
      "outputs": [],
      "source": [
        "df.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52op_lT1JGBm"
      },
      "source": [
        "Insert cells to find the median of the high temperature and the mode of the high temperature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SzYabOyjd88"
      },
      "source": [
        "Now insert a cell to show the medians of all the features in df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNAWruWRnX2W"
      },
      "source": [
        "For Geometric means use the stat library in scipy (note: geometric mean is not suited for mean temperature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDj1ZGQCng2G"
      },
      "outputs": [],
      "source": [
        "from scipy.stats.mstats import gmean\n",
        "gmean(df['high degree F'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSaU8mWQpcx-"
      },
      "source": [
        "Likewise harmonic mean (also not relevant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atxsy0tmpiHZ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats.mstats import hmean\n",
        "hmean(df['high degree F'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9N4CTWpy-D"
      },
      "source": [
        "Moving on to Measures of Dispersion or Variation\n",
        "\n",
        "First, for range we can use numpy \"peak to peak\" function ptp() to compute ranges. Alternatively, we could in Pandas use df.max() and df.min() and a little math would give the same result,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-TiLsh3thD0"
      },
      "outputs": [],
      "source": [
        "np.ptp(df['high degree F'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NhDPgSHxfkd"
      },
      "source": [
        "Next use df.var() to compute variance.  You adjust variance for population vs sample by using the ddof (delta degrees of freedom) parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZcKcMkryNxt"
      },
      "outputs": [],
      "source": [
        "# sample variance\n",
        "df['high degree F'].var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glf3TPBWyrXJ"
      },
      "outputs": [],
      "source": [
        "#population variance\n",
        "df['high degree F'].var(ddof=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmNPs8y3zBjP"
      },
      "source": [
        "For standard deviation, the code is similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H372tQfgzIaP"
      },
      "outputs": [],
      "source": [
        "# sample standard deviation\n",
        "df['high degree F'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbtigQkkzYRh"
      },
      "outputs": [],
      "source": [
        "#popuation standard variation\n",
        "df['high degree F'].std(ddof=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUJ2wx7kUwB"
      },
      "source": [
        "Ranking.  Percent rank uses df.rank(). First lets see the high temperatures for the month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6OLerMqZbuA"
      },
      "outputs": [],
      "source": [
        "df['high degree F']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8__eQbzfZ4SJ"
      },
      "source": [
        "Now for each of those readings, rank shows the percentile rank of that value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_6PdHhnjSe2"
      },
      "outputs": [],
      "source": [
        "df['high degree F'].rank(pct=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltDGcq_MlkUr"
      },
      "source": [
        "Now quantile rank to get data quartiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urV_h8d9lsKm"
      },
      "outputs": [],
      "source": [
        "pd.qcut(df['low degree F'], q=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esg-_RR3tgrM"
      },
      "source": [
        "Finally, a summary of the characteristics of the dataframe which includes many of the statistics can be displayed by using describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nFH4p6SgGhz"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTKenn1Rnmf_"
      },
      "source": [
        "More sophisticated distribution analysis and ploting can be done with the Dataframe data and Matplotlib. For example to compare high and low temperature distributionshistograms (and Gaussian Kernel Density Estimates as distribution model) we can use this plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zbK-uJ_-aiW"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, ax = pyplot.subplots()\n",
        "df[[\"high degree F\", \"low degree F\"]].plot.kde(ax=ax, legend=False, title='Nov 2019 College Station Daily Temperature')\n",
        "df[[\"high degree F\", \"low degree F\"]].plot.hist(density=True, ax=ax, alpha=0.3)\n",
        "ax.set_ylabel('Probability')\n",
        "ax.grid(axis='y')\n",
        "ax.set_facecolor('#d8dcd6')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX9w5WtnpGk1"
      },
      "source": [
        "PART 2: Loading and Cleaning Leaf Blower Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF8yBykQpFz6"
      },
      "outputs": [],
      "source": [
        "# change the directory as needed\n",
        "!ls drive/MyDrive/ECEN250_LeafBlowers.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4teTzmwpmOv"
      },
      "outputs": [],
      "source": [
        "# importing dataset\n",
        "df = pd.read_csv('drive/MyDrive/ECEN250_Lab2_LeafBlowers.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR3yUmaLpsHQ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe7GsWlGqBY3"
      },
      "source": [
        "Let's look at what we read in to make sure it's what we expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5hTshB9px0F"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGeRQk_JyzkQ"
      },
      "source": [
        "Manufacturer, model, retail, and source are going to be non-numeric by nature. These are currently objects, because some entries may be numeric and some string. We will leave them as they are since they will not be used in our statistics or ML data analysis. The feature motor type contains strings: brushed, brushless, or unspecified. It is currently an object datatype. This feature -- is categorical.  Is it nominal or ordinal?? We will use it a lot in\n",
        "our analysis so let's turn it into a numerical that we can manipulate. We can do that with the following python cell (notice when we use flag inplace=True we are directly modifying our df -- if not done inplace replace() returns a new modified df):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6tv9VFtk13U"
      },
      "outputs": [],
      "source": [
        "df['motor type'].replace(['brushless', 'unspecified', 'brushed'],\n",
        "                        [0, 0.5, 1], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdw5FGuOlXoa"
      },
      "source": [
        "Let's verify this worked by doing df.head(10) again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZJifHY4ldxU"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC4bJFJRqYuF"
      },
      "source": [
        "To make sure it it indeed now numeric, let's do df.info() again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXuQZ6AxqlEM"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELOMj7KNqqhc"
      },
      "source": [
        "Good, so now everything we will use today is numeric!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjDzffTRl0Xv"
      },
      "source": [
        "If for some reason this didn't work -- or you decided that you want to just make them categorical for now and numeric later, go back to the cell that imports the csv file.  Re-execute that cell and all the cells up to the one you are changing.  THIS IS WHY WE DON'T MODIFY OUR CSV SOURCE FILE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9tWLLQOmZVk"
      },
      "source": [
        "Take another look at the results of the df.info() above -- the non-null counts will differ if you have some csv rows where you have not filled in each field.  In the dataframe those entries will be NaN. Our statistics and modeling will be messed up by those NaN values. If you do not have 50 entries in which all\n",
        "features have a value, you need to stop now and find a few more to add to your csv.  Then restart the runtime and rerun your notebook to here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr8kFoMrRUl6"
      },
      "source": [
        "If you have NaNs in your dataframe you can use the techniques you learned earlier to fix them here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTaGnSGi3Apa"
      },
      "source": [
        "Use df.info() to verify no remaining NaNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojBT8c_y3AOe"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EgWgd0iifLM"
      },
      "source": [
        "We are almost done --- we first should write out the data from our dataframe since we will use this cleaned data for Lab 3.  To do that you can write out your data into a CSV.  The flag index=False prevents the Index values (column 0) from being written to the csv.  We do this because read_csv by default assigns index values as the data is being read from the csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aalqlEkJjjza"
      },
      "outputs": [],
      "source": [
        "df.to_csv('drive/MyDrive/ECEN250_Lab2_LeafBlowersClean.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJvzUPD9kJrX"
      },
      "source": [
        "Now make sure that this notebook shows all your executed cells.  If it does not, you can restart the runtime and runall or you can restart the runtime and individually reexecute all of your cells.\n",
        "\n",
        "Now download your notebook as both an ipynb file (use a name that includes your name please) and a PDF file (to create the PDF in Colab, go to File > Print and select Save as PDF), and upload both files to Canvas for your submission for Lab 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kf1mrc9q6PQ"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}